# GPU-Accelerated Robotics Matrix Multiplication Performance Analysis

Comprehensive benchmarking suite for small batched 4×4 matrix multiplications on CPU, OpenMP, and CUDA GPU, with focus on robotic kinematics applications. This project investigates memory transfer optimizations, memory layout strategies, and scalability across varying chain lengths to enable real-time forward/inverse kinematics computations.

## Project Overview

In robotic applications such as articulated arm or legged robot control, forward and inverse kinematics rely heavily on repeated small matrix multiplications to compute transformations between coordinate frames. Each joint's position and orientation is represented by 4×4 homogeneous transformation matrices, and evaluating the end-effector's pose involves chaining these matrices together.

This project systematically analyzes three critical aspects of GPU acceleration for robotics workloads:

1. **Memory Transfer Performance**: Comparing pageable vs. pinned vs. managed memory strategies
2. **Memory Layout Optimization**: Evaluating separate vs. interleaved matrix storage
3. **Variable Chain Length Scaling**: Performance analysis across different kinematic chain lengths (2-32 joints)

## Key Findings

### 1. Memory Transfer Performance (compare_mem_access)
- **Pinned memory achieves ~2.4× speedup** over pageable memory for both H2D and D2H transfers
- Tested data sizes: 64 MB to 1024 MB
- Unified/managed memory shows negligible overhead on integrated GPUs
- **Result**: Pinned memory (`cudaHostAlloc`) is essential for robotics real-time constraints

### 2. Memory Layout Optimization (compare_mem_layout)
- **Combined (interleaved) layout achieves 1.7× CPU speedup** over separate arrays due to better cache locality
- GPU performance is comparable for both layouts (~13ms total time for 1M matrices)
- Memory transfer time dominates (95%+) for typical robotics workloads
- **Result**: Combined layout optimizes CPU fallback without GPU penalty

### 3. Variable Chain Length Scaling (compare_variable_joints)
- **GPU maintains 3.4-4.9× speedup** over single-threaded CPU across all chain lengths (2-32 joints)
- Kernel GFLOPS: GPU achieves 300-730 GFLOPS vs CPU 3-12 GFLOPS
- Transfer overhead scales linearly with chain length but kernel efficiency remains high
- Energy efficiency: GPU delivers 1.4-1.6 GFLOPS/Watt vs CPU 0.05 GFLOPS/Watt
- **Result**: GPU acceleration is effective even for small robotics problems

## Benchmarks

The project includes three comprehensive benchmarks:

### 1. `compare_mem_access` - Memory Transfer Analysis
Compares pageable (`new[]`), pinned (`cudaHostAlloc`), and managed (`cudaMallocManaged`) memory:
```bash
make compare_mem_access
./compare_mem_access 256  # Test with 256 MB
```
**Outputs**: H2D/D2H transfer times and bandwidth (GB/s) to CSV

### 2. `compare_mem_layout` - Memory Layout Comparison
Compares separate arrays (A, B, C, D) vs. combined interleaved layout:
```bash
make compare_mem_layout
./compare_mem_layout 500000 64  # 500K matrices, 64 threads/block
```
**Outputs**: CPU, OpenMP, GPU execution times and GFLOPS to CSV

### 3. `compare_variable_joints` - Chain Length Scaling
Tests performance across 2-32 joints (variable chain length):
```bash
make compare_variable_joints
./compare_variable_joints 500000 64  # 500K operations, 64 threads/block
```
**Outputs**: Execution time, GFLOPS, power consumption, energy efficiency to CSV

## Quick Start

```bash
# Build all benchmarks
make

# Run individual benchmarks
make run_mem_access
make run_mem_layout  
make run_joints

# View results
python3 -m jupyter notebook memory_comparison.ipynb
```

## Visualization

The `memory_comparison.ipynb` Jupyter notebook provides comprehensive analysis:
- Memory transfer bandwidth comparison charts
- Execution time vs. configuration plots
- GPU speedup analysis across chain lengths
- Kernel vs. transfer time breakdown
- Power consumption and energy efficiency metrics
- GFLOPS per Watt energy efficiency comparisons

## System Requirements

- CUDA Toolkit (12.0+)
- C++11 compiler with OpenMP support
- Python 3.x with pandas, matplotlib, seaborn (for visualization)
- `nvidia-smi` (optional, for auto-detection)

## Auto-Detection

The Makefile automatically:
1. Detects `x86_64` vs `aarch64` CPU architecture via `uname -m`
2. Queries GPU compute capability (SM) via `nvidia-smi --query-gpu=compute_cap`
3. Falls back to sensible defaults if detection fails

### Manual GPU Override

```bash
make SM=75            # Turing (RTX 20xx)
make SM=86            # Ampere (RTX 30xx)
make SM=87            # Jetson Orin
make SM=89            # Ada Lovelace (RTX 40xx)
```

## Performance Results Summary

Based on extensive benchmarking (10 runs per configuration, averaged):

| Metric | CPU | CPU+OMP | GPU (Pinned) | GPU (Managed) |
|--------|-----|---------|--------------|---------------|
| **Avg Execution Time** (500K ops) | 91.8 ms | 7.3 ms | 30.0 ms | 30.2 ms |
| **Avg GFLOPS** | 11.8 | 166.1 | 354.9 | 353.2 |
| **Avg Power** | 228 W | 221 W | 223 W | 224 W |
| **Energy Efficiency** | 0.05 GFLOPS/W | 0.75 GFLOPS/W | 1.59 GFLOPS/W | 1.58 GFLOPS/W |
| **Speedup vs CPU** | 1.0× | 12.6× | 3.4× | 3.4× |

**Key Insights**:
- **Memory transfer dominates** GPU time (90%+ for typical workloads)
- **OpenMP** delivers best raw performance for this workload size
- **GPU** provides best energy efficiency (32× better than CPU)
- **Pinned memory** reduces transfer time by 2.4× vs. pageable memory

## Technical Implementation

The Makefile automatically:
1. Detects `x86_64` vs `aarch64` via `uname -m`
2. Queries GPU SM via `nvidia-smi --query-gpu=compute_cap`
3. Falls back to sensible defaults if detection fails

## Performance Tips

- **Small batches (<1K)**: CPU single-threaded is fastest
- **Medium batches (1K-100K)**: OpenMP provides good speedup
- **Large batches (100K+)**: GPU kernel dominates (if data stays on GPU)
- **Memory transfer overhead**: Dominates GPU total time for small workloads

## Build from Scratch

```bash
make clean
make rebuild
```

## Requirements

- CUDA Toolkit (12.0+)
- C++11 compiler
- OpenMP support (`-fopenmp`)
- `nvidia-smi` (for auto-detection, optional)

## Motivation
- In robotic applications such as articulated arm or legged robot control, forward and inverse kinematics rely heavily on repeated small matrix multiplications to compute transformations between coordinate frames. For instance, each joint’s position and orientation are represented by 4×4 homogeneous transformation matrices, and evaluating the end-effector’s pose involves chaining these matrices together. During trajectory interpolation, thousands of forward/inverse kinematics computations must be performed per second to achieve smooth motion. These operations consist primarily of small matrix multiplications that can become computational bottlenecks on embedded CPUs. Therefore, exploring efficient, scalable methods for this operation — particularly on GPUs — is crucial for enabling real-time robotic control and motion planning. 

## Methodology

### Workload

- Operation: For each row i in a batch, compute (Aᵢ×Bᵢ)×(Cᵢ×Dᵢ) where A, B, C, D are 4×4 matrices stored in row-major order.
- FLOPs: One 4×4 matmul is 64 MACs = 128 FLOPs. We perform 3 matmuls per row, so 384 FLOPs per row and 3×128×N total.
- Data: Inputs are initialized with a fixed PRNG seed (uniform in [-1, 1]) for reproducibility.
- Layout: Each matrix is 16 contiguous floats; the i-th matrices start at offset i×16 in their respective arrays.

### Implementations

1) CPU (single-thread)
- Straightforward triple-loop 4×4 multiply used as a building block.
- Batched evaluation iterates rows sequentially and materializes intermediate products (A×B) and (C×D) on the stack per row.

2) CPU (OpenMP)
- Identical math to the single-thread baseline, parallelized with `#pragma omp parallel for` over rows.
- Each thread computes its own intermediates on the stack to avoid synchronization and false sharing.

3) CUDA GPU
- One thread processes one row (i.e., the full (A×B)×(C×D) chain for that row).
- Kernel uses a fully unrolled 4×4 routine (`__device__ __forceinline__`) to keep all temporaries in registers; no shared memory is required for this small problem size.
- Launch configuration: a 1D thread arrangement along y; `threadsPerBlock = 64` by default and `numBlocks = ceil(N / threadsPerBlock)`.
- Memory access: Each thread reads 16 floats from A, B, C, and D at contiguous offsets; across threads these reads are strided by 16 floats, which yields coalesced 32B/128B transactions on modern GPUs when arrays are aligned.

### Build and Auto-Detection

- The Makefile probes CPU ISA via `uname -m` (x86_64 vs aarch64) and queries GPU compute capability (SM) via `nvidia-smi`.
- Flags are set accordingly (e.g., `-arch=sm_${SM}` for `nvcc`, `-fopenmp` for CPU OpenMP).
- Users can override detection with `make SM=86` (see Manual Overrides).

### Timing and Metrics

- CPU timers: `std::chrono::high_resolution_clock` around the full batched loops for both single-thread and OpenMP.
- GPU timers: host-side wall clock split into three segments with `cudaDeviceSynchronize()` to bound the kernel portion:
	- Copy host→device
	- Kernel execution
	- Copy device→host
- Total GPU time includes transfers; “kernel-only” time excludes transfers. This highlights the transfer overhead for small batches.
- Throughput (GFLOPS) is computed as total_FLOPs / elapsed_seconds / 1e9, with total_FLOPs = 3×128×N.
- Note: CUDA events (`cudaEventRecord`) would provide finer-grained device-side kernel timing; host-side timing with synchronization is sufficient for this benchmark’s comparisons.

### Correctness Verification

- We compare GPU results against CPU single-thread output element-wise with a tolerance of 1e-4.
- To reduce host memory usage on very large runs, we retain up to 10,000 elements from the CPU result for verification while freeing the full buffer.
- Any mismatch reports the first differing index and values to aid debugging.

### Experiment Protocol

- Batch sizes: run provided presets (`run-small`, `run`, `run-medium`, `run-large`) to observe scaling across regimes where CPU, OpenMP, or GPU dominate.
- Threading: OpenMP uses `omp_get_max_threads()` by default; you can control threads via `OMP_NUM_THREADS`.
- Warm-up: No explicit warm-up is performed; on systems with cold GPU clocks, consider an initial throwaway run for more stable numbers.
- Repeats: For publication-quality numbers, average multiple runs and report mean ± stdev.

### Interpreting Results

- Small batches (<1K): kernel launch and PCIe transfers dominate; CPU often wins.
- Medium batches (1K–100K): OpenMP typically provides a strong speedup over single-thread and may compete with GPU depending on transfer cost.
- Large batches (≥100K): GPU kernel time dominates overall and generally yields the best throughput if data residency on the device is maintained.

## Reproducibility

- Deterministic inputs: A fixed PRNG seed is used to initialize matrices for repeatable results.
- System snapshot: Use the included target to log system info alongside results.

```bash
# Capture system info used during the run
make info | tee system_info.txt

# Optional: capture compiler and driver details
nvcc --version | tee -a system_info.txt
nvidia-smi -q | tee -a system_info.txt
uname -a | tee -a system_info.txt
```

- Threading and affinity: Control OpenMP parallelism with `OMP_NUM_THREADS` and consider pinning threads (e.g., `GOMP_CPU_AFFINITY` on GCC).
- Run protocol: For stable numbers, perform a short warm-up run, then take the mean ± stdev over several repetitions.

## Limitations and Future Work

- Timing granularity: Host-side timing with `cudaDeviceSynchronize()` is sufficient for high-level comparisons but not cycle-accurate. Future: use CUDA events for device-side kernel timing and report both.
- Transfer overhead: For small batches, PCIe transfers dominate. Future: use pinned (page-locked) memory and CUDA streams to overlap H2D/D2H with compute; consider keeping data resident on the GPU for iterative workloads.
- Kernel launch overhead: Very small N is sensitive to launch latency. Future: CUDA Graphs can reduce launch overhead.
- Memory layout: Current layout is row-major batches of 4×4. Future: experiment with structure-of-arrays (SoA) or interleaving to improve coalescing on large N.
- Math libraries: Add a cuBLAS strided batched SGEMM baseline for reference on small GEMMs.
- Tensor cores: Explore WMMA/CUTLASS packing strategies to leverage tensor cores for 4×4 blocks when applicable.
- CPU vectorization: Add explicit SIMD (e.g., AVX2/AVX-512/NEON) and compiler flags (`-O3 -march=native`) with microbenchmarks.

## Troubleshooting

- `nvidia-smi: command not found` or no devices shown: Ensure the NVIDIA driver is installed and the host sees the GPU. On Jetson, `nvidia-smi` may be limited; set `SM` manually (see Manual Overrides).
- `nvcc fatal: Unsupported gpu architecture 'sm_XY'`: Choose a supported SM for your CUDA Toolkit or upgrade CUDA.
- OpenMP not enabled: Make sure your compiler supports `-fopenmp` and your toolchain links the OpenMP runtime.
- Runtime mismatches in verification: Verify drivers and toolkit versions; try a slightly larger tolerance (e.g., 1e-3) to account for different math orders on some platforms.
- Out-of-memory for huge runs: Reduce N or run the preset sizes; GPU memory must accommodate all four input batches and one output batch.

## Project Layout

- `small_matmul.cpp`: Host benchmark harness (CPU, OpenMP, and CUDA integration, timing, verification, reporting)
- `small_matmul.cu`: CUDA kernel and launch wrapper for batched 4×4 matmul
- `README.md`: This document

If you want a cuBLAS baseline or event-based timing, open an issue or add a PR—both are straightforward extensions to this setup.
